{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/e5/f1/d9251b565fce9f8daeb45611e3e0d2f7f248429e40908dcee3b6fe1b5944/openai-2.11.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-2.11.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ragac\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Obtaining dependency information for jiter<1,>=0.10.0 from https://files.pythonhosted.org/packages/bb/60/1032b30ae0572196b0de0e87dce3b6c26a1eff71aad5fe43dee3082d32e0/jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for h11>=0.16 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-2.11.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.1/1.1 MB 1.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.3/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.3/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.3/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.3/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.4/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.5/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.6/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.8/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 0.9/1.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 0.9/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 0.9/1.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.0/1.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.0/1.1 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.0/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.1/1.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "   ---------------------------------------- 0.0/73.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 73.5/73.5 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.8/78.8 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "   ---------------------------------------- 0.0/204.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 61.4/204.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 61.4/204.9 kB ? eta -:--:--\n",
      "   ------------- ------------------------- 71.7/204.9 kB 653.6 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 92.2/204.9 kB 521.8 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 112.6/204.9 kB 502.0 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 122.9/204.9 kB 450.6 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 122.9/204.9 kB 450.6 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 153.6/204.9 kB 398.2 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 153.6/204.9 kB 398.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 174.1/204.9 kB 360.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 174.1/204.9 kB 360.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 204.9/204.9 kB 365.9 kB/s eta 0:00:00\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: jiter, h11, distro, httpcore, httpx, openai\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "Successfully installed distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.12.0 openai-2.11.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "040ff534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d5b5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint cargado: https://rafae-mj2b6wcp-eastus2.cognitiveservices.azure.com\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "print(\"Endpoint cargado:\", endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cf378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear cliente de AzureOpenAI\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",          # el que sale en tu snippet\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f93841da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Nombre del deployment que creaste en Azure\n",
    "MODEL_NAME = \"gpt-5-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77e73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta del modelo:\n",
      "\n",
      "No tengo acceso directo a tu entorno ni a tus credenciales, así que no puedo confirmar de manera definitiva si estás usando **Azure OpenAI** o el servicio estándar de **OpenAI**.  \n",
      "\n",
      "Sin embargo, puedes verificarlo tú mismo de las siguientes formas:\n",
      "\n",
      "1. **Revisa la URL del endpoint**:  \n",
      "   - Si la dirección que usas para hacer las solicitudes tiene el formato  \n",
      "     `https://{nombre-de-tu-recurso}.openai.azure.com/...`,  \n",
      "     entonces estás usando **Azure OpenAI**.  \n",
      "   - Si la URL es `https://api.openai.com/...`, estás usando **OpenAI** directamente.\n",
      "\n",
      "2. **Consulta tu configuración o portal**:  \n",
      "   - En **Azure Portal**, si ves un recurso llamado *Azure OpenAI Service*, eso confirma que estás usando esa plataforma.\n",
      "\n",
      "3. **Comprueba las variables de entorno o las credenciales**:  \n",
      "   - En Azure se usan las claves de recurso (`AZURE_OPEN\n"
     ]
    }
   ],
   "source": [
    "# 4. Hacer una llamada de prueba\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"You are a helpful assistant that answers in Spanish.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"Hola, ¿me puedes confirmar que ya estoy usando Azure OpenAI?\"},\n",
    "    ],\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(\"\\nRespuesta del modelo:\\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb274e",
   "metadata": {},
   "source": [
    "## Simple Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e73f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunResult:\n",
    "    model: str\n",
    "    task_id: str\n",
    "    prompt: str\n",
    "    output: str\n",
    "    latency_s: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "def call_model(model: str, messages: List[Dict[str, str]]) -> RunResult:\n",
    "    \"\"\"\n",
    "    Llama al modelo de Azure y regresa un objeto con el texto + métricas básicas.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,      # aquí va el nombre del deployment\n",
    "        messages=messages\n",
    "    )\n",
    "    t1 = time.time()\n",
    "\n",
    "    choice = resp.choices[0]\n",
    "    usage = resp.usage\n",
    "\n",
    "    return RunResult(\n",
    "        model=model,\n",
    "        task_id=\"\",\n",
    "        prompt=messages[-1][\"content\"],\n",
    "        output=choice.message.content,\n",
    "        latency_s=t1 - t0,\n",
    "        input_tokens=usage.prompt_tokens,\n",
    "        output_tokens=usage.completion_tokens,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "697db951",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_TASKS = [\n",
    "    {\n",
    "        \"id\": \"food_1\",\n",
    "        \"instruction\": \"Extrae ÚNICAMENTE el nombre de la comida mencionada en esta reseña.\",\n",
    "        \"input\": \"The tacos al pastor were amazing but the service was slow.\",\n",
    "        \"expected\": \"tacos al pastor\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lastname_1\",\n",
    "        \"instruction\": \"Devuelve ÚNICAMENTE el apellido o apellidos de este nombre completo.\",\n",
    "        \"input\": \"Rafael Gallegos Cortés\",\n",
    "        \"expected\": \"Gallegos Cortés\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"legal_1\",\n",
    "        \"instruction\": (\n",
    "            \"Identifica los nombres de entidades legales en el texto y \"\n",
    "            \"devuélvelos como una lista de cadenas (por ejemplo: \"\n",
    "            '[\"Entidad 1\", \"Entidad 2\"]).'\n",
    "        ),\n",
    "        \"input\": \"El contrato se celebra entre Pemex Exploración y Producción, S.A. de C.V. y CFE.\",\n",
    "        \"expected\": '[\"Pemex Exploración y Producción, S.A. de C.V.\", \"CFE\"]',\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_MODELS = [\n",
    "    \"gpt-5-chat\",\n",
    "    \"o4-mini\", \n",
    "    \"DeepSeek-V3.1\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0d4bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    return text.strip().strip('\"').lower()\n",
    "\n",
    "def eval_simple_tasks():\n",
    "    \"\"\"\n",
    "    Corre todas las tareas simples para todos los modelos en SIMPLE_MODELS.\n",
    "    Devuelve una lista de (RunResult, expected_output).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for model in SIMPLE_MODELS:\n",
    "        for task in SIMPLE_TASKS:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"Eres un motor de extracción muy preciso.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{task['instruction']}\\n\\nTexto: {task['input']}\",\n",
    "                },\n",
    "            ]\n",
    "            r = call_model(model, messages)\n",
    "            r.task_id = task[\"id\"]\n",
    "            results.append((r, task[\"expected\"]))\n",
    "    return results\n",
    "\n",
    "def summarize_simple(results):\n",
    "    \"\"\"\n",
    "    Calcula accuracy, latencia promedio y tokens promedio por modelo.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(lambda: {\n",
    "        \"n\": 0, \"correct\": 0, \"latency_sum\": 0.0,\n",
    "        \"input_tokens\": 0, \"output_tokens\": 0,\n",
    "    })\n",
    "\n",
    "    for r, expected in results:\n",
    "        s = stats[r.model]\n",
    "        s[\"n\"] += 1\n",
    "        s[\"latency_sum\"] += r.latency_s\n",
    "        s[\"input_tokens\"] += r.input_tokens\n",
    "        s[\"output_tokens\"] += r.output_tokens\n",
    "\n",
    "        if \"legal\" in r.task_id:\n",
    "            # comparación muy simple; luego la podemos mejorar parseando JSON\n",
    "            ok = normalize(r.output).replace(\" \", \"\") == normalize(expected).replace(\" \", \"\")\n",
    "        else:\n",
    "            ok = normalize(r.output) == normalize(expected)\n",
    "\n",
    "        if ok:\n",
    "            s[\"correct\"] += 1\n",
    "\n",
    "    summary = []\n",
    "    for model, s in stats.items():\n",
    "        acc = s[\"correct\"] / s[\"n\"] if s[\"n\"] > 0 else 0.0\n",
    "        avg_latency = s[\"latency_sum\"] / s[\"n\"] if s[\"n\"] > 0 else 0.0\n",
    "        avg_tokens = (s[\"input_tokens\"] + s[\"output_tokens\"]) / s[\"n\"] if s[\"n\"] > 0 else 0.0\n",
    "        summary.append({\n",
    "            \"model\": model,\n",
    "            \"accuracy\": acc,\n",
    "            \"avg_latency_s\": avg_latency,\n",
    "            \"avg_tokens\": avg_tokens,\n",
    "        })\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58d86aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'gpt-5-chat',\n",
       "  'accuracy': 1.0,\n",
       "  'avg_latency_s': 0.5267310937245687,\n",
       "  'avg_tokens': 69.33333333333333},\n",
       " {'model': 'o4-mini',\n",
       "  'accuracy': 1.0,\n",
       "  'avg_latency_s': 2.426086187362671,\n",
       "  'avg_tokens': 235.66666666666666},\n",
       " {'model': 'DeepSeek-V3.1',\n",
       "  'accuracy': 1.0,\n",
       "  'avg_latency_s': 0.4136193593343099,\n",
       "  'avg_tokens': 72.0}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = eval_simple_tasks()\n",
    "summary = summarize_simple(results)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc973714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task_id</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input_text</th>\n",
       "      <th>expected</th>\n",
       "      <th>output</th>\n",
       "      <th>correct</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>food_1</td>\n",
       "      <td>Extrae ÚNICAMENTE el nombre de la comida menci...</td>\n",
       "      <td>The tacos al pastor were amazing but the servi...</td>\n",
       "      <td>tacos al pastor</td>\n",
       "      <td>tacos al pastor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.711</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>lastname_1</td>\n",
       "      <td>Devuelve ÚNICAMENTE el apellido o apellidos de...</td>\n",
       "      <td>Rafael Gallegos Cortés</td>\n",
       "      <td>Gallegos Cortés</td>\n",
       "      <td>Gallegos Cortés</td>\n",
       "      <td>True</td>\n",
       "      <td>0.322</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>legal_1</td>\n",
       "      <td>Identifica los nombres de entidades legales en...</td>\n",
       "      <td>El contrato se celebra entre Pemex Exploración...</td>\n",
       "      <td>[\"Pemex Exploración y Producción, S.A. de C.V....</td>\n",
       "      <td>[\"Pemex Exploración y Producción, S.A. de C.V....</td>\n",
       "      <td>True</td>\n",
       "      <td>0.547</td>\n",
       "      <td>82</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>food_1</td>\n",
       "      <td>Extrae ÚNICAMENTE el nombre de la comida menci...</td>\n",
       "      <td>The tacos al pastor were amazing but the servi...</td>\n",
       "      <td>tacos al pastor</td>\n",
       "      <td>tacos al pastor</td>\n",
       "      <td>True</td>\n",
       "      <td>1.784</td>\n",
       "      <td>50</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>lastname_1</td>\n",
       "      <td>Devuelve ÚNICAMENTE el apellido o apellidos de...</td>\n",
       "      <td>Rafael Gallegos Cortés</td>\n",
       "      <td>Gallegos Cortés</td>\n",
       "      <td>Gallegos Cortés</td>\n",
       "      <td>True</td>\n",
       "      <td>2.126</td>\n",
       "      <td>42</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>legal_1</td>\n",
       "      <td>Identifica los nombres de entidades legales en...</td>\n",
       "      <td>El contrato se celebra entre Pemex Exploración...</td>\n",
       "      <td>[\"Pemex Exploración y Producción, S.A. de C.V....</td>\n",
       "      <td>[\"Pemex Exploración y Producción, S.A. de C.V....</td>\n",
       "      <td>True</td>\n",
       "      <td>3.368</td>\n",
       "      <td>81</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>food_1</td>\n",
       "      <td>Extrae ÚNICAMENTE el nombre de la comida menci...</td>\n",
       "      <td>The tacos al pastor were amazing but the servi...</td>\n",
       "      <td>tacos al pastor</td>\n",
       "      <td>tacos al pastor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.401</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>lastname_1</td>\n",
       "      <td>Devuelve ÚNICAMENTE el apellido o apellidos de...</td>\n",
       "      <td>Rafael Gallegos Cortés</td>\n",
       "      <td>Gallegos Cortés</td>\n",
       "      <td>Gallegos Cortés</td>\n",
       "      <td>True</td>\n",
       "      <td>0.361</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>legal_1</td>\n",
       "      <td>Identifica los nombres de entidades legales en...</td>\n",
       "      <td>El contrato se celebra entre Pemex Exploración...</td>\n",
       "      <td>[\"Pemex Exploración y Producción, S.A. de C.V....</td>\n",
       "      <td>[\"Pemex Exploración y Producción, S.A. de C.V....</td>\n",
       "      <td>True</td>\n",
       "      <td>0.479</td>\n",
       "      <td>86</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model     task_id  \\\n",
       "0     gpt-5-chat      food_1   \n",
       "1     gpt-5-chat  lastname_1   \n",
       "2     gpt-5-chat     legal_1   \n",
       "3        o4-mini      food_1   \n",
       "4        o4-mini  lastname_1   \n",
       "5        o4-mini     legal_1   \n",
       "6  DeepSeek-V3.1      food_1   \n",
       "7  DeepSeek-V3.1  lastname_1   \n",
       "8  DeepSeek-V3.1     legal_1   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Extrae ÚNICAMENTE el nombre de la comida menci...   \n",
       "1  Devuelve ÚNICAMENTE el apellido o apellidos de...   \n",
       "2  Identifica los nombres de entidades legales en...   \n",
       "3  Extrae ÚNICAMENTE el nombre de la comida menci...   \n",
       "4  Devuelve ÚNICAMENTE el apellido o apellidos de...   \n",
       "5  Identifica los nombres de entidades legales en...   \n",
       "6  Extrae ÚNICAMENTE el nombre de la comida menci...   \n",
       "7  Devuelve ÚNICAMENTE el apellido o apellidos de...   \n",
       "8  Identifica los nombres de entidades legales en...   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  The tacos al pastor were amazing but the servi...   \n",
       "1                             Rafael Gallegos Cortés   \n",
       "2  El contrato se celebra entre Pemex Exploración...   \n",
       "3  The tacos al pastor were amazing but the servi...   \n",
       "4                             Rafael Gallegos Cortés   \n",
       "5  El contrato se celebra entre Pemex Exploración...   \n",
       "6  The tacos al pastor were amazing but the servi...   \n",
       "7                             Rafael Gallegos Cortés   \n",
       "8  El contrato se celebra entre Pemex Exploración...   \n",
       "\n",
       "                                            expected  \\\n",
       "0                                    tacos al pastor   \n",
       "1                                    Gallegos Cortés   \n",
       "2  [\"Pemex Exploración y Producción, S.A. de C.V....   \n",
       "3                                    tacos al pastor   \n",
       "4                                    Gallegos Cortés   \n",
       "5  [\"Pemex Exploración y Producción, S.A. de C.V....   \n",
       "6                                    tacos al pastor   \n",
       "7                                    Gallegos Cortés   \n",
       "8  [\"Pemex Exploración y Producción, S.A. de C.V....   \n",
       "\n",
       "                                              output  correct  latency_s  \\\n",
       "0                                    tacos al pastor     True      0.711   \n",
       "1                                    Gallegos Cortés     True      0.322   \n",
       "2  [\"Pemex Exploración y Producción, S.A. de C.V....     True      0.547   \n",
       "3                                    tacos al pastor     True      1.784   \n",
       "4                                    Gallegos Cortés     True      2.126   \n",
       "5  [\"Pemex Exploración y Producción, S.A. de C.V....     True      3.368   \n",
       "6                                    tacos al pastor     True      0.401   \n",
       "7                                    Gallegos Cortés     True      0.361   \n",
       "8  [\"Pemex Exploración y Producción, S.A. de C.V....     True      0.479   \n",
       "\n",
       "   input_tokens  output_tokens  \n",
       "0            51              5  \n",
       "1            43              6  \n",
       "2            82             21  \n",
       "3            50             87  \n",
       "4            42            152  \n",
       "5            81            295  \n",
       "6            51              5  \n",
       "7            46              6  \n",
       "8            86             22  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapa de metadatos de cada tarea por id\n",
    "TASK_META = {t[\"id\"]: t for t in SIMPLE_TASKS}\n",
    "\n",
    "rows = []\n",
    "for r, expected in results:\n",
    "    meta = TASK_META.get(r.task_id, {})\n",
    "    \n",
    "    # misma lógica de \"correct\" que usamos en summarize_simple\n",
    "    if \"legal\" in r.task_id:\n",
    "        correct = normalize(r.output).replace(\" \", \"\") == normalize(expected).replace(\" \", \"\")\n",
    "    else:\n",
    "        correct = normalize(r.output) == normalize(expected)\n",
    "    \n",
    "    rows.append({\n",
    "        \"model\": r.model,\n",
    "        \"task_id\": r.task_id,\n",
    "        \"instruction\": meta.get(\"instruction\", \"\"),\n",
    "        \"input_text\": meta.get(\"input\", \"\"),\n",
    "        \"expected\": expected,\n",
    "        \"output\": r.output,\n",
    "        \"correct\": correct,\n",
    "        \"latency_s\": round(r.latency_s, 3),\n",
    "        \"input_tokens\": r.input_tokens,\n",
    "        \"output_tokens\": r.output_tokens,\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c08afda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Modelo:   gpt-5-chat\n",
      "Tarea:    food_1\n",
      "Instrucción:\n",
      "Extrae ÚNICAMENTE el nombre de la comida mencionada en esta reseña.\n",
      "\n",
      "Texto de entrada:\n",
      "The tacos al pastor were amazing but the service was slow.\n",
      "\n",
      "Esperado: tacos al pastor\n",
      "Output:   tacos al pastor\n",
      "Correcto: True\n",
      "Latencia: 0.711 s | tokens in/out: 51/5\n",
      "\n",
      "============================================================\n",
      "Modelo:   gpt-5-chat\n",
      "Tarea:    lastname_1\n",
      "Instrucción:\n",
      "Devuelve ÚNICAMENTE el apellido o apellidos de este nombre completo.\n",
      "\n",
      "Texto de entrada:\n",
      "Rafael Gallegos Cortés\n",
      "\n",
      "Esperado: Gallegos Cortés\n",
      "Output:   Gallegos Cortés\n",
      "Correcto: True\n",
      "Latencia: 0.322 s | tokens in/out: 43/6\n",
      "\n",
      "============================================================\n",
      "Modelo:   gpt-5-chat\n",
      "Tarea:    legal_1\n",
      "Instrucción:\n",
      "Identifica los nombres de entidades legales en el texto y devuélvelos como una lista de cadenas (por ejemplo: [\"Entidad 1\", \"Entidad 2\"]).\n",
      "\n",
      "Texto de entrada:\n",
      "El contrato se celebra entre Pemex Exploración y Producción, S.A. de C.V. y CFE.\n",
      "\n",
      "Esperado: [\"Pemex Exploración y Producción, S.A. de C.V.\", \"CFE\"]\n",
      "Output:   [\"Pemex Exploración y Producción, S.A. de C.V.\", \"CFE\"]\n",
      "Correcto: True\n",
      "Latencia: 0.547 s | tokens in/out: 82/21\n",
      "\n",
      "============================================================\n",
      "Modelo:   o4-mini\n",
      "Tarea:    food_1\n",
      "Instrucción:\n",
      "Extrae ÚNICAMENTE el nombre de la comida mencionada en esta reseña.\n",
      "\n",
      "Texto de entrada:\n",
      "The tacos al pastor were amazing but the service was slow.\n",
      "\n",
      "Esperado: tacos al pastor\n",
      "Output:   tacos al pastor\n",
      "Correcto: True\n",
      "Latencia: 1.784 s | tokens in/out: 50/87\n",
      "\n",
      "============================================================\n",
      "Modelo:   o4-mini\n",
      "Tarea:    lastname_1\n",
      "Instrucción:\n",
      "Devuelve ÚNICAMENTE el apellido o apellidos de este nombre completo.\n",
      "\n",
      "Texto de entrada:\n",
      "Rafael Gallegos Cortés\n",
      "\n",
      "Esperado: Gallegos Cortés\n",
      "Output:   Gallegos Cortés\n",
      "Correcto: True\n",
      "Latencia: 2.126 s | tokens in/out: 42/152\n",
      "\n",
      "============================================================\n",
      "Modelo:   o4-mini\n",
      "Tarea:    legal_1\n",
      "Instrucción:\n",
      "Identifica los nombres de entidades legales en el texto y devuélvelos como una lista de cadenas (por ejemplo: [\"Entidad 1\", \"Entidad 2\"]).\n",
      "\n",
      "Texto de entrada:\n",
      "El contrato se celebra entre Pemex Exploración y Producción, S.A. de C.V. y CFE.\n",
      "\n",
      "Esperado: [\"Pemex Exploración y Producción, S.A. de C.V.\", \"CFE\"]\n",
      "Output:   [\"Pemex Exploración y Producción, S.A. de C.V.\", \"CFE\"]\n",
      "Correcto: True\n",
      "Latencia: 3.368 s | tokens in/out: 81/295\n",
      "\n",
      "============================================================\n",
      "Modelo:   DeepSeek-V3.1\n",
      "Tarea:    food_1\n",
      "Instrucción:\n",
      "Extrae ÚNICAMENTE el nombre de la comida mencionada en esta reseña.\n",
      "\n",
      "Texto de entrada:\n",
      "The tacos al pastor were amazing but the service was slow.\n",
      "\n",
      "Esperado: tacos al pastor\n",
      "Output:   tacos al pastor\n",
      "Correcto: True\n",
      "Latencia: 0.401 s | tokens in/out: 51/5\n",
      "\n",
      "============================================================\n",
      "Modelo:   DeepSeek-V3.1\n",
      "Tarea:    lastname_1\n",
      "Instrucción:\n",
      "Devuelve ÚNICAMENTE el apellido o apellidos de este nombre completo.\n",
      "\n",
      "Texto de entrada:\n",
      "Rafael Gallegos Cortés\n",
      "\n",
      "Esperado: Gallegos Cortés\n",
      "Output:   Gallegos Cortés\n",
      "Correcto: True\n",
      "Latencia: 0.361 s | tokens in/out: 46/6\n",
      "\n",
      "============================================================\n",
      "Modelo:   DeepSeek-V3.1\n",
      "Tarea:    legal_1\n",
      "Instrucción:\n",
      "Identifica los nombres de entidades legales en el texto y devuélvelos como una lista de cadenas (por ejemplo: [\"Entidad 1\", \"Entidad 2\"]).\n",
      "\n",
      "Texto de entrada:\n",
      "El contrato se celebra entre Pemex Exploración y Producción, S.A. de C.V. y CFE.\n",
      "\n",
      "Esperado: [\"Pemex Exploración y Producción, S.A. de C.V.\", \"CFE\"]\n",
      "Output:   [\"Pemex Exploración y Producción, S.A. de C.V.\", \"CFE\"]\n",
      "Correcto: True\n",
      "Latencia: 0.479 s | tokens in/out: 86/22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in df_results.iterrows():\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Modelo:   {row['model']}\")\n",
    "    print(f\"Tarea:    {row['task_id']}\")\n",
    "    print(f\"Instrucción:\\n{row['instruction']}\")\n",
    "    print(f\"\\nTexto de entrada:\\n{row['input_text']}\")\n",
    "    print(f\"\\nEsperado: {row['expected']}\")\n",
    "    print(f\"Output:   {row['output']}\")\n",
    "    print(f\"Correcto: {row['correct']}\")\n",
    "    print(f\"Latencia: {row['latency_s']} s | tokens in/out: {row['input_tokens']}/{row['output_tokens']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af013df",
   "metadata": {},
   "source": [
    "## Analytical tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc6e68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunResult:\n",
    "    model: str\n",
    "    task_id: str\n",
    "    prompt: str\n",
    "    output: str\n",
    "    latency_s: float\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "\n",
    "def call_model(model: str, messages: List[Dict[str, str]], max_tokens: int | None = None) -> RunResult:\n",
    "    \"\"\"\n",
    "    Llama al modelo de Azure y regresa un objeto con el texto + métricas básicas.\n",
    "    Para gpt-5-chat usamos 'max_completion_tokens' en lugar de 'max_tokens'.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    kwargs = {}\n",
    "    if max_tokens is not None:\n",
    "        # Azure GPT-5 usa este nombre de parámetro:\n",
    "        kwargs[\"max_completion_tokens\"] = max_tokens\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs,\n",
    "    )\n",
    "    t1 = time.time()\n",
    "\n",
    "    choice = resp.choices[0]\n",
    "    usage = resp.usage\n",
    "\n",
    "    return RunResult(\n",
    "        model=model,\n",
    "        task_id=\"\",\n",
    "        prompt=messages[-1][\"content\"],\n",
    "        output=choice.message.content,\n",
    "        latency_s=t1 - t0,\n",
    "        input_tokens=usage.prompt_tokens,\n",
    "        output_tokens=usage.completion_tokens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b3bdf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYTICAL_TASKS = [\n",
    "    {\n",
    "        \"id\": \"missing_1\",\n",
    "        \"description\": \"Missing values analysis\",\n",
    "        \"prompt\": \"\"\"\n",
    "You are a senior data analyst. I have this dataset summary (in CSV):\n",
    "\n",
    "column,missing_count,mean,std,min,max\n",
    "age,10,35,7,18,65\n",
    "income,50,25000,12000,5000,90000\n",
    "city,0,NA,NA,NA,NA\n",
    "\n",
    "1) Describe the missing value pattern.\n",
    "2) Propose a reasonable imputation strategy.\n",
    "3) Mention risks or caveats.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ts_1\",\n",
    "        \"description\": \"Time series analysis\",\n",
    "        \"prompt\": \"\"\"\n",
    "You are a time series expert. I have this monthly revenue series (index, month, value):\n",
    "\n",
    "1,2023-01,100\n",
    "2,2023-02,110\n",
    "3,2023-03,130\n",
    "4,2023-04,140\n",
    "5,2023-05,160\n",
    "6,2023-06,170\n",
    "7,2023-07,200\n",
    "8,2023-08,210\n",
    "9,2023-09,180\n",
    "10,2023-10,220\n",
    "11,2023-11,230\n",
    "12,2023-12,250\n",
    "\n",
    "1) Describe trend and any anomalies.\n",
    "2) Give a short qualitative forecast for the next 3 months.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"stats_1\",\n",
    "        \"description\": \"Statistical insight on multivariate data\",\n",
    "        \"prompt\": \"\"\"\n",
    "You are a senior data scientist. I have this dataset summary:\n",
    "\n",
    "- 1000 rows.\n",
    "- Variables:\n",
    "    * age (numeric, 18-80, mean=40, std=12)\n",
    "    * income (numeric, strongly right-skewed, many values close to 0, some very large)\n",
    "    * churn (binary: 1 if customer left, 0 otherwise)\n",
    "    * segment (categorical: A, B, C)\n",
    "\n",
    "Tasks:\n",
    "1) Propose 2-3 hypotheses that would be interesting to test statistically.\n",
    "2) Explain briefly which statistical methods you would use for each hypothesis.\n",
    "3) Suggest 2 simple visualizations to better understand the data.\n",
    "\"\"\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "049b4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYTICAL_MODELS = [\n",
    "    \"gpt-5-chat\",\n",
    "    \"o4-mini\", \n",
    "    \"DeepSeek-V3.1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df1d668d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_analytical_tasks():\n",
    "    rows = []\n",
    "    for model in ANALYTICAL_MODELS:\n",
    "        for task in ANALYTICAL_TASKS:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert data analyst.\"},\n",
    "                {\"role\": \"user\", \"content\": task[\"prompt\"]},\n",
    "            ]\n",
    "            # r = call_model(model, messages, max_tokens=600) No funciona el maximo para algunos modelos\n",
    "            r = call_model(model, messages)\n",
    "            r.task_id = task[\"id\"]\n",
    "            rows.append(r)\n",
    "    return rows\n",
    "\n",
    "results_analytical = run_analytical_tasks()\n",
    "len(results_analytical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4979822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>mimic_eda_missing</td>\n",
       "      <td>\\nYou are a senior data analyst working with I...</td>\n",
       "      <td>Let's go through this systematically, as a sen...</td>\n",
       "      <td>32.067313</td>\n",
       "      <td>23630</td>\n",
       "      <td>1918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>mimic_modeling</td>\n",
       "      <td>\\nYou are a senior data scientist helping to b...</td>\n",
       "      <td>Let’s walk through this step-by-step as a seni...</td>\n",
       "      <td>25.011250</td>\n",
       "      <td>2278</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>mimic_eda_missing</td>\n",
       "      <td>\\nYou are a senior data analyst working with I...</td>\n",
       "      <td>1) Overall structure  \\n- Rows: one ICU stay e...</td>\n",
       "      <td>25.575531</td>\n",
       "      <td>23629</td>\n",
       "      <td>2778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>mimic_modeling</td>\n",
       "      <td>\\nYou are a senior data scientist helping to b...</td>\n",
       "      <td>1) Baseline modeling approach  \\n• Start with ...</td>\n",
       "      <td>10.883580</td>\n",
       "      <td>2277</td>\n",
       "      <td>1452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>mimic_eda_missing</td>\n",
       "      <td>\\nYou are a senior data analyst working with I...</td>\n",
       "      <td>Of course. As a senior data analyst working wi...</td>\n",
       "      <td>34.630941</td>\n",
       "      <td>20615</td>\n",
       "      <td>1894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>mimic_modeling</td>\n",
       "      <td>\\nYou are a senior data scientist helping to b...</td>\n",
       "      <td>Of course. As a senior data scientist, here is...</td>\n",
       "      <td>21.290509</td>\n",
       "      <td>2040</td>\n",
       "      <td>1781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model            task_id  \\\n",
       "0     gpt-5-chat  mimic_eda_missing   \n",
       "1     gpt-5-chat     mimic_modeling   \n",
       "2        o4-mini  mimic_eda_missing   \n",
       "3        o4-mini     mimic_modeling   \n",
       "4  DeepSeek-V3.1  mimic_eda_missing   \n",
       "5  DeepSeek-V3.1     mimic_modeling   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  \\nYou are a senior data analyst working with I...   \n",
       "1  \\nYou are a senior data scientist helping to b...   \n",
       "2  \\nYou are a senior data analyst working with I...   \n",
       "3  \\nYou are a senior data scientist helping to b...   \n",
       "4  \\nYou are a senior data analyst working with I...   \n",
       "5  \\nYou are a senior data scientist helping to b...   \n",
       "\n",
       "                                              output  latency_s  input_tokens  \\\n",
       "0  Let's go through this systematically, as a sen...  32.067313         23630   \n",
       "1  Let’s walk through this step-by-step as a seni...  25.011250          2278   \n",
       "2  1) Overall structure  \\n- Rows: one ICU stay e...  25.575531         23629   \n",
       "3  1) Baseline modeling approach  \\n• Start with ...  10.883580          2277   \n",
       "4  Of course. As a senior data analyst working wi...  34.630941         20615   \n",
       "5  Of course. As a senior data scientist, here is...  21.290509          2040   \n",
       "\n",
       "   output_tokens  \n",
       "0           1918  \n",
       "1           1484  \n",
       "2           2778  \n",
       "3           1452  \n",
       "4           1894  \n",
       "5           1781  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytical = pd.DataFrame([r.__dict__ for r in results_analytical])\n",
    "df_analytical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "72114dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados analíticos a CSV\n",
    "df_analytical.to_excel(\"analytical_results.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca1273",
   "metadata": {},
   "source": [
    "### Statistics and Missing values from Mimic and Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54f87c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic\n",
    "# train = pd.read_csv('mimic_train.csv')\n",
    "# train.head(2)\n",
    "\n",
    "# --- MIMIC: ICU mortality dataset ---\n",
    "mimic_df = pd.read_csv(\"mimic_train.csv\")\n",
    "\n",
    "# muestra pequeña\n",
    "mimic_sample = mimic_df.head(100).to_string(index=False)\n",
    "\n",
    "# describe solo numéricas (para no llenar demasiado)\n",
    "mimic_summary = mimic_df.describe(include=\"number\").to_string()\n",
    "\n",
    "# conteo de missing\n",
    "mimic_missing_df = mimic_df.isna().sum().reset_index()\n",
    "mimic_missing_df.columns = [\"column\", \"missing_count\"]\n",
    "mimic_missing = mimic_missing_df.to_string(index=False)\n",
    "\n",
    "# distribución básica del target\n",
    "if \"HOSPITAL_EXPIRE_FLAG\" in mimic_df.columns:\n",
    "    mimic_target_dist = mimic_df[\"HOSPITAL_EXPIRE_FLAG\"].value_counts(normalize=True).to_frame(\"proportion\").to_string()\n",
    "else:\n",
    "    mimic_target_dist = \"Column HOSPITAL_EXPIRE_FLAG not found.\"\n",
    "\n",
    "\n",
    "# --- TITANIC: classic survival dataset ---\n",
    "titanic_df = pd.read_csv(\"titanic3.csv\")\n",
    "\n",
    "titanic_sample = titanic_df.head(100).to_string(index=False)\n",
    "titanic_summary = titanic_df.describe(include=\"number\").to_string()\n",
    "\n",
    "titanic_missing_df = titanic_df.isna().sum().reset_index()\n",
    "titanic_missing_df.columns = [\"column\", \"missing_count\"]\n",
    "titanic_missing = titanic_missing_df.to_string(index=False)\n",
    "\n",
    "# distribución de survival si existe\n",
    "if \"survived\" in titanic_df.columns:\n",
    "    titanic_target_dist = titanic_df[\"survived\"].value_counts(normalize=True).to_frame(\"proportion\").to_string()\n",
    "else:\n",
    "    titanic_target_dist = \"Column survived not found.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64263933",
   "metadata": {},
   "source": [
    "### Mimic Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3064385",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYTICAL_TASKS = [\n",
    "\n",
    "###################################################################################################################################\n",
    "#############################TAREA 1. Missing values and EDA in MIMIC dataset\n",
    "    {\n",
    "        \"id\": \"mimic_eda_missing\",\n",
    "        \"description\": \"EDA and missing values in ICU mortality dataset (MIMIC)\",\n",
    "        \"prompt\": f\"\"\"\n",
    "You are a senior data analyst working with ICU data (MIMIC-III style).\n",
    "We want to predict in-hospital mortality for ICU patients (HOSPITAL_EXPIRE_FLAG).\n",
    "\n",
    "Here is the sample dataset:\n",
    "\n",
    "{mimic_sample}\n",
    "\n",
    "Here is the count of missing values per column:\n",
    "\n",
    "{mimic_missing}\n",
    "\n",
    "Here is a summary of basic statistics for numeric variables:\n",
    "\n",
    "{mimic_summary}\n",
    "\n",
    "Here is the distribution of the target HOSPITAL_EXPIRE_FLAG (proportion):\n",
    "\n",
    "{mimic_target_dist}\n",
    "\n",
    "Tasks:\n",
    "1) Describe the overall structure of the dataset (types of variables, what they seem to represent).\n",
    "2) Analyze the missing value pattern: which variables are more problematic and what might be the underlying reasons in an ICU context.\n",
    "3) Propose concrete strategies to handle missing values (e.g., dropping rows/columns, different imputations) and justify them.\n",
    "4) Mention at least 3 potential risks or caveats, especially regarding data leakage and bias in a medical setting.\n",
    "\"\"\"\n",
    "    },\n",
    "\n",
    "##################################################################################################################################\n",
    "##################################TAREA 2. Modeling design for MIMIC dataset\n",
    "\n",
    "{\n",
    "        \"id\": \"mimic_modeling\",\n",
    "        \"description\": \"Model design for ICU mortality prediction (MIMIC)\",\n",
    "        \"prompt\": f\"\"\"\n",
    "You are a senior data scientist helping to build a model that predicts in-hospital mortality\n",
    "for ICU patients using the variable HOSPITAL_EXPIRE_FLAG as the target.\n",
    "\n",
    "You have access to the same dataset as before, with vitals, demographics, and other ICU-related variables.\n",
    "\n",
    "You know the following:\n",
    "- The dataset has missing values as shown below (counts per column):\n",
    "\n",
    "{mimic_missing}\n",
    "\n",
    "- The numeric variables have the following basic statistics:\n",
    "\n",
    "{mimic_summary}\n",
    "\n",
    "- The target distribution (HOSPITAL_EXPIRE_FLAG) is:\n",
    "\n",
    "{mimic_target_dist}\n",
    "\n",
    "Tasks:\n",
    "1) Propose a reasonable baseline modeling approach (e.g., logistic regression, tree-based model, etc.) and explain why.\n",
    "2) Describe how you would preprocess the data: handling missing values, scaling, encoding categorical variables, and dealing with highly correlated features.\n",
    "3) Discuss how you would handle potential class imbalance in HOSPITAL_EXPIRE_FLAG.\n",
    "4) Suggest appropriate evaluation metrics for this medical prediction problem and explain why they are suitable (e.g., ROC-AUC, PR-AUC, calibration).\n",
    "5) Mention at least 2 ways to check whether the model might be unfair or biased toward some patient subgroups.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9a33e45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_analytical_tasks():\n",
    "    rows = []\n",
    "    for model in ANALYTICAL_MODELS:\n",
    "        for task in ANALYTICAL_TASKS:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert data analyst.\"},\n",
    "                {\"role\": \"user\", \"content\": task[\"prompt\"]},\n",
    "            ]\n",
    "            # r = call_model(model, messages, max_tokens=800) No funciona el maximo para algunos modelos\n",
    "            r = call_model(model, messages)\n",
    "            r.task_id = task[\"id\"]\n",
    "            rows.append(r)\n",
    "    return rows\n",
    "\n",
    "results_analytical = run_analytical_tasks()\n",
    "len(results_analytical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f81d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>mimic_eda_missing</td>\n",
       "      <td>\\nYou are a senior data analyst working with I...</td>\n",
       "      <td>Let’s address each of your tasks systematicall...</td>\n",
       "      <td>27.254901</td>\n",
       "      <td>23630</td>\n",
       "      <td>1579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>mimic_modeling</td>\n",
       "      <td>\\nYou are a senior data scientist helping to b...</td>\n",
       "      <td>Let’s address each of the five tasks systemati...</td>\n",
       "      <td>22.382430</td>\n",
       "      <td>2278</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>mimic_eda_missing</td>\n",
       "      <td>\\nYou are a senior data analyst working with I...</td>\n",
       "      <td>1) Overall structure  \\n- 20885 ICU stays (row...</td>\n",
       "      <td>10.047299</td>\n",
       "      <td>23629</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>mimic_modeling</td>\n",
       "      <td>\\nYou are a senior data scientist helping to b...</td>\n",
       "      <td>1) Baseline modeling approach  \\n• Logistic re...</td>\n",
       "      <td>8.655360</td>\n",
       "      <td>2277</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>mimic_eda_missing</td>\n",
       "      <td>\\nYou are a senior data analyst working with I...</td>\n",
       "      <td>### 1) Dataset Structure and Variable Types\\n\\...</td>\n",
       "      <td>18.056446</td>\n",
       "      <td>20615</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model            task_id  \\\n",
       "0     gpt-5-chat  mimic_eda_missing   \n",
       "1     gpt-5-chat     mimic_modeling   \n",
       "2        o4-mini  mimic_eda_missing   \n",
       "3        o4-mini     mimic_modeling   \n",
       "4  DeepSeek-V3.1  mimic_eda_missing   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  \\nYou are a senior data analyst working with I...   \n",
       "1  \\nYou are a senior data scientist helping to b...   \n",
       "2  \\nYou are a senior data analyst working with I...   \n",
       "3  \\nYou are a senior data scientist helping to b...   \n",
       "4  \\nYou are a senior data analyst working with I...   \n",
       "\n",
       "                                              output  latency_s  input_tokens  \\\n",
       "0  Let’s address each of your tasks systematicall...  27.254901         23630   \n",
       "1  Let’s address each of the five tasks systemati...  22.382430          2278   \n",
       "2  1) Overall structure  \\n- 20885 ICU stays (row...  10.047299         23629   \n",
       "3  1) Baseline modeling approach  \\n• Logistic re...   8.655360          2277   \n",
       "4  ### 1) Dataset Structure and Variable Types\\n\\...  18.056446         20615   \n",
       "\n",
       "   output_tokens  \n",
       "0           1579  \n",
       "1           1408  \n",
       "2           1083  \n",
       "3           1075  \n",
       "4           1065  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytical = pd.DataFrame([r.__dict__ for r in results_analytical])\n",
    "df_analytical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9b85bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mimic\n",
    "df_analytical.to_excel(\"analytical_results_mimic.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d68d6",
   "metadata": {},
   "source": [
    "### Titanic Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "58763abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYTICAL_TASKS = [\n",
    "\n",
    "##################################################################################################################################\n",
    "##################################TAREA 1. Missing values and EDA in Titanic dataset\n",
    "\n",
    "{\n",
    "        \"id\": \"titanic_missing\",\n",
    "        \"description\": \"Missing values and basic EDA in Titanic survival dataset\",\n",
    "        \"prompt\": f\"\"\"\n",
    "You are a data analyst working with the Titanic passenger dataset.\n",
    "\n",
    "Here is a sample of the data (first 5 rows):\n",
    "\n",
    "{titanic_sample}\n",
    "\n",
    "Here is the count of missing values per column:\n",
    "\n",
    "{titanic_missing}\n",
    "\n",
    "Here is a summary of basic statistics for numeric variables:\n",
    "\n",
    "{titanic_summary}\n",
    "\n",
    "Here is the distribution of the target 'survived' (proportion):\n",
    "\n",
    "{titanic_target_dist}\n",
    "\n",
    "Tasks:\n",
    "1) Identify the most relevant variables with missing values (e.g., age, cabin, embarked) and describe how they might affect the analysis.\n",
    "2) Propose at least two different imputation strategies for the 'age' variable and discuss pros and cons of each.\n",
    "3) Propose a reasonable way to handle the 'cabin' variable given that it has many missing values and a large number of categories.\n",
    "4) Suggest 2–3 visualizations that would help understand the relationship between survival and key variables (e.g., sex, class, age).\n",
    "\"\"\"\n",
    "    },\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "##################################TAREA 2. Modeling design for Titanic dataset\n",
    "\n",
    "    {\n",
    "        \"id\": \"titanic_modeling\",\n",
    "        \"description\": \"Model design for predicting survival on Titanic dataset\",\n",
    "        \"prompt\": f\"\"\"\n",
    "You are a senior data scientist helping to build a model that predicts passenger survival\n",
    "on the Titanic dataset (target variable: 'survived').\n",
    "\n",
    "You have the following information:\n",
    "\n",
    "- Sample of the dataset:\n",
    "\n",
    "{titanic_sample}\n",
    "\n",
    "- Numeric summary:\n",
    "\n",
    "{titanic_summary}\n",
    "\n",
    "- Missing value counts:\n",
    "\n",
    "{titanic_missing}\n",
    "\n",
    "- Target distribution:\n",
    "\n",
    "{titanic_target_dist}\n",
    "\n",
    "Tasks:\n",
    "1) Propose a baseline modeling approach (e.g., logistic regression) and one more flexible model (e.g., tree-based model) and explain the intuition behind both.\n",
    "2) Explain how you would preprocess the data: encoding categorical variables (e.g., sex, class), handling missing values, and possibly engineering new features.\n",
    "3) Suggest which evaluation metrics you would use and why (e.g., accuracy, F1-score, ROC-AUC).\n",
    "4) Mention 2–3 model diagnostics or validation strategies you would apply (e.g., cross-validation, learning curves, calibration plots).\n",
    "\"\"\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9821db44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_analytical = run_analytical_tasks()\n",
    "len(results_analytical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f3a7f314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>task_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>titanic_missing</td>\n",
       "      <td>\\nYou are a data analyst working with the Tita...</td>\n",
       "      <td>Let's go step-by-step carefully.\\n\\n---\\n\\n## ...</td>\n",
       "      <td>29.840174</td>\n",
       "      <td>6256</td>\n",
       "      <td>1569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-chat</td>\n",
       "      <td>titanic_modeling</td>\n",
       "      <td>\\nYou are a senior data scientist helping to b...</td>\n",
       "      <td>Let's go step by step.\\n\\n---\\n\\n## **1) Basel...</td>\n",
       "      <td>26.915074</td>\n",
       "      <td>6255</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>titanic_missing</td>\n",
       "      <td>\\nYou are a data analyst working with the Tita...</td>\n",
       "      <td>1) Missing-data overview and implications  \\n ...</td>\n",
       "      <td>14.000849</td>\n",
       "      <td>6255</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o4-mini</td>\n",
       "      <td>titanic_modeling</td>\n",
       "      <td>\\nYou are a senior data scientist helping to b...</td>\n",
       "      <td>1) Modeling approaches  \\n- Baseline: Logistic...</td>\n",
       "      <td>9.388242</td>\n",
       "      <td>6254</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-V3.1</td>\n",
       "      <td>titanic_missing</td>\n",
       "      <td>\\nYou are a data analyst working with the Tita...</td>\n",
       "      <td>Of course. As a data analyst, I'll address eac...</td>\n",
       "      <td>21.626806</td>\n",
       "      <td>5406</td>\n",
       "      <td>1526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model           task_id  \\\n",
       "0     gpt-5-chat   titanic_missing   \n",
       "1     gpt-5-chat  titanic_modeling   \n",
       "2        o4-mini   titanic_missing   \n",
       "3        o4-mini  titanic_modeling   \n",
       "4  DeepSeek-V3.1   titanic_missing   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  \\nYou are a data analyst working with the Tita...   \n",
       "1  \\nYou are a senior data scientist helping to b...   \n",
       "2  \\nYou are a data analyst working with the Tita...   \n",
       "3  \\nYou are a senior data scientist helping to b...   \n",
       "4  \\nYou are a data analyst working with the Tita...   \n",
       "\n",
       "                                              output  latency_s  input_tokens  \\\n",
       "0  Let's go step-by-step carefully.\\n\\n---\\n\\n## ...  29.840174          6256   \n",
       "1  Let's go step by step.\\n\\n---\\n\\n## **1) Basel...  26.915074          6255   \n",
       "2  1) Missing-data overview and implications  \\n ...  14.000849          6255   \n",
       "3  1) Modeling approaches  \\n- Baseline: Logistic...   9.388242          6254   \n",
       "4  Of course. As a data analyst, I'll address eac...  21.626806          5406   \n",
       "\n",
       "   output_tokens  \n",
       "0           1569  \n",
       "1           1400  \n",
       "2           1420  \n",
       "3           1206  \n",
       "4           1526  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analytical = pd.DataFrame([r.__dict__ for r in results_analytical])\n",
    "df_analytical.to_csv(\"analytical_results_titanic.csv\", index=False)\n",
    "df_analytical.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd326e",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff26824",
   "metadata": {},
   "source": [
    "##  Chat desde la linea de entrada del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e30bd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_chat(model: str = \"gpt-5-chat\"):\n",
    "    \"\"\"\n",
    "    Mini chatbot en consola usando tu deployment de Azure.\n",
    "    Mantiene el historial de la conversación.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Eres un asistente experto en análisis de datos, estadística, \"\n",
    "                \"valores faltantes y series de tiempo. Responde SIEMPRE en español.\"\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"=== Mini chatbot con Azure (modelo: {}) ===\".format(model))\n",
    "    print(\"Escribe 'salir', 'exit' o 'quit' para terminar.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user = input(\"Tú: \")\n",
    "        if user.strip().lower() in (\"salir\", \"exit\", \"quit\"):\n",
    "            print(\"Bot: ¡Hasta luego! 👋\")\n",
    "            break\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_completion_tokens=300,  # O ajusta si quieres respuestas más cortas/largas\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"\\nBot: {answer}\\n\")\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b24050e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mini chatbot con Azure (modelo: gpt-5-chat) ===\n",
      "Escribe 'salir', 'exit' o 'quit' para terminar.\n",
      "\n",
      "\n",
      "Bot: ¡Hola! 😊 Todo muy bien, gracias. ¿Y tú qué tal? ¿En qué tema relacionado con análisis de datos o series de tiempo te gustaría que te ayude hoy?\n",
      "\n",
      "\n",
      "Bot: Buena pregunta 😊  \n",
      "\n",
      "Funciono como un modelo de lenguaje entrenado con grandes volúmenes de texto. Eso me permite entender el contexto de lo que escribes y generar respuestas coherentes, explicaciones, ejemplos de código, pasos de análisis, etc.  \n",
      "\n",
      "En términos simples:  \n",
      "\n",
      "1. **Recibo tu entrada (texto)** → analizo la estructura, el tema y lo que estás pidiendo.  \n",
      "2. **Genero una representación interna** del contexto, aplicando mis conocimientos (por ejemplo, estadística, imputación de valores faltantes, modelado de series de tiempo, etc.).  \n",
      "3. **Produzco una respuesta** en español, procurando que sea clara, útil y ajustada a tu nivel de detalle.  \n",
      "\n",
      "No tengo acceso a internet ni a bases de datos en tiempo real, pero sí cuento con un conocimiento acumulado hasta 2024.  \n",
      "\n",
      "¿Quieres que te explique cómo aplico esto en un flujo de trabajo de **análisis de datos**?\n",
      "\n",
      "\n",
      "Bot: Excelente pregunta 👏  \n",
      "\n",
      "Hay varias formas de “especializarme” o de adaptar mis respuestas a un **tema o tarea en particular**, aunque técnicamente no puedo aprender cosas nuevas fuera de esta conversación. Lo que sí puedes hacer es **guiar mi comportamiento y foco** mediante el contexto que me proporcionas.  \n",
      "\n",
      "Aquí te dejo algunas estrategias efectivas:  \n",
      "\n",
      "1. **Define el rol que quieres que asuma.**  \n",
      "   Ejemplo:  \n",
      "   > “Actúa como un analista de datos senior con experiencia en imputación de valores faltantes.”  \n",
      "   Eso hace que todas mis respuestas se alineen con ese rol.  \n",
      "\n",
      "2. **Proporciona el contexto del problema.**  \n",
      "   Cuéntame qué datos tienes, cuál es el objetivo del análisis o el tipo de serie de tiempo.  \n",
      "   Cuanto más contexto, mejores recomendaciones puedo darte.  \n",
      "\n",
      "3. **Establece el nivel de profundidad o formato.**  \n",
      "   Por ejemplo:  \n",
      "   > “Explícalo como si estuviera preparando un informe para dirección.”  \n",
      "   o  \n",
      "   > “Muéstrame un ejemplo con código en Python.”  \n",
      "\n",
      "4. **Usa instrucciones persistentes.**  \n",
      "   Si me pides que siempre mantenga cierto enfoque (por ejemplo, priorizar la interpretación estadística o evitar detalles de programación), puedo seguir esa línea durante toda la sesión.  \n",
      "\n",
      "¿Quieres que defina un “modo de trabajo” para las próximas respuestas? Por ejemplo\n",
      "\n",
      "\n",
      "Bot: Sí, tengo un **límite de tokens**, y es una parte importante de cómo funciono internamente.  \n",
      "\n",
      "Te explico brevemente:  \n",
      "\n",
      "- Un **token** es una unidad mínima de texto (puede ser una palabra corta, parte de una palabra o un símbolo).  \n",
      "  Por ejemplo, la frase “hola, ¿cómo estás?” tiene unos 5‑6 tokens, según el modelo.  \n",
      "\n",
      "- Cada interacción (tu pregunta + mi respuesta + el contexto previo) se procesa dentro de un **límite máximo de tokens** que depende del modelo con el que estés interactuando.  \n",
      "  Si el límite se supera, el modelo **no puede recordar** partes anteriores de la conversación o **corta la respuesta**.  \n",
      "\n",
      "- Este límite no es visible para el usuario dentro de una charla normal, pero típicamente está en el rango de **miles de tokens** (por ejemplo, 8 000, 16 000 o más, según la versión).  \n",
      "\n",
      "En la práctica, mientras las conversaciones no sean muy extensas o las respuestas no sean demasiado largas, **no tenés que preocuparte demasiado** por los tokens.  \n",
      "\n",
      "¿Querés que te explique cómo influye ese límite cuando analizamos datos extensos o series de tiempo largas?\n",
      "\n",
      "Bot: ¡Hasta luego! 👋\n"
     ]
    }
   ],
   "source": [
    "mini_chat(\"gpt-5-chat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae90b61",
   "metadata": {},
   "source": [
    "## Chatbot desde la terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6401f",
   "metadata": {},
   "source": [
    "Correr el script de **\"chat_terminal.py\"** con la terminal desde la carpeta de excelexorcist project el siguiente comando:\n",
    "\n",
    "\"python chat_terminal.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d7ac0",
   "metadata": {},
   "source": [
    "## Chatbot desde streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\ragac\\anaconda3\\lib\\site-packages (1.46.1)\n",
      "Requirement already satisfied: openai in c:\\users\\ragac\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ragac\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (5.3.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (8.0.4)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (23.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\ragac\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (11.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (15.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (2.32.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (4.14.1)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from streamlit) (6.3.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.16)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ragac\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install streamlit openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35620dd",
   "metadata": {},
   "source": [
    "Correr el script **\"app.py\"** con la terminal desde la carpeta de excelexorcist project el siguiente comando:\n",
    "\n",
    "streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
